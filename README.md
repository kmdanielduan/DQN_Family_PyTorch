# Deep Q-Learning in PyTorch

This is a repository of DQN and its variants implementation in PyTorch based on the original papar.

## Algorithms

Algorithms below will be implemented in this repository.

**Deep Q Network (DQN)** from Playing Atari with Deep Reinforcement Learning, Mnih et al, 2013. [[arxiv](https://arxiv.org/abs/1312.5602v1)] [[summary](https://github.com/kmdanielduan/Key-Paper-Summary-in-DRL/blob/master/01.%20Model-Free%20RL/%5B001%5D%20Playing%20Atari%20with%20Deep%20Reinforcement%20Learning.md)]

**Double DQN** Deep Reinforcement Learning with Double Q-learning, Hasselt et al 2015. [[arxiv](https://arxiv.org/abs/1509.06461)] [[summary](https://github.com/kmdanielduan/Key-Paper-Summary-in-DRL/blob/master/01.%20Model-Free%20RL/%5B004%5D%20Deep%20Reinforcement%20Learning%20with%20Double%20Q-learning.md)]

**Dueling DQN** Dueling Network Architectures for Deep Reinforcement Learning, Wang et al, 2015. [[arxiv](https://arxiv.org/abs/1511.06581)] [[summary](https://github.com/kmdanielduan/Key-Paper-Summary-in-DRL/blob/master/01.%20Model-Free%20RL/%5B003%5D%20Dueling%20Network%20Architectures%20for%20Deep%20Reinforcement%20Learning.md)]

**Prioritized Experience Replay (PER)** Prioritized Experience Replay, Schaul et al, 2015.  [[arxiv](https://arxiv.org/abs/1511.05952)] [[summary](https://github.com/kmdanielduan/Key-Paper-Summary-in-DRL/blob/master/01.%20Model-Free%20RL/%5B005%5D%20Prioritized%20Experience%20Replay.md)]

## Methods

### DQN

The Q-network I use here is 3-hidden-layer perceptrons(MLP). The hidden_size is 32.

```python
class DQN(nn.Module):
    def __init__(self, num_actions, input_size, hidden_size):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, num_actions)
```

The corresponding policy action is generated by <img src="https://latex.codecogs.com/svg.latex?\large&space;\epsilon" title="\large \epsilon" />-greedy method. <img src="https://latex.codecogs.com/svg.latex?\large&space;\epsilon" title="\large \epsilon" />is exponentially  decayed w.r.t. a designated decay rate.

When evaluating the performance of the model, I wrote a class method called `demo`. `demo` basically plays the game for 100 times by exploiting the actions generated by the policy network (equivalent to <img src="https://latex.codecogs.com/svg.latex?\large&space;\epsilon" title="\large \epsilon" /> = 1.0), and get the average score of the games as the score of the current policy network.

The policy network scores, average scores of the past 10 versions of policy network, and the current episode duration are plotted.

## Experiments and Results

### DQN

```python
# typical hyperparameters
class AgentConfig:
    EXPERIMENT_NO = 9

    START_EPISODE = 0
    NUM_EPISODES = 1000
    MEMORY_CAPA = 100000
    MAX_EPS = 1.0
    MIN_EPS = 0.05
    UPDATE_FREQ = 10
    RES_PATH = './experiments/'
    PER = False
    
    LR = 5e-4
    DECAY_RATE = 0.999
    BATCH_SIZE = 16
    GAMMA = 0.999

class EnvConfig:
    ENV = "CartPole-v0"
```

Below are the experiments to test DQN under different circumstances. In the plot,

*Blue line:* policy network scores

*Orange line:* the current episode score under greedy policy

*Green line:* average scores of the past 10 versions of policy network

| No.  | learning rate | decay rate | batch size |   gamma   | result.png                       | Comments                                                     |
| :--: | :-----------: | :--------: | :--------: | :-------: | -------------------------------- | ------------------------------------------------------------ |
|  1   |     5e-4      |    0.99    |     32     |   0.99    | ![1-result](assets/1-result.png) | Solved the game after around 800 episodes.                   |
|  2   |   **1e-4**    |    0.99    |     32     |   0.99    | ![2-result](assets/2-result.png) | Small learning rate makes the network hard to learn anything. Not converging at all. |
|  3   |   **1e-3**    |    0.99    |     32     |   0.99    | ![3-result](assets/3-result.png) | Solved the game after around 200 episodes. Displayed a pattern of high probability of divergence due to high learning rate. |
|  4   |     5e-4      |    0.99    |   **64**   |   0.99    | ![4-result](assets/4-result.png) | Bigger batch size counter-intuitively leads to worse performance. |
|  5   |     5e-4      |    0.99    |   **16**   |   0.99    | ![5-result](assets/5-result.png) | Smaller batch size makes the  policy stay in high performance but much more noise. |
|  6   |     5e-4      |    0.99    |   **8**    |   0.99    | ![6-result](assets/6-result.png) | Smaller batch size makes the  policy stay in high performance but much more noise. |
|  7   |     5e-4      |    0.99    |     16     | **0.999** | ![7-result](assets/7-result.png) | Higher gamma means preservation of the past learned knowledge. Solved the game around 100 episodes, and stayed there for around 80 episodes. |
|  8   |     5e-4      |    0.99    |     16     |  **0.9**  | ![8-result](assets/8-result.png) | Lower gamma leads to high variations of the performance.     |
|  9   |     5e-4      | **0.999**  |     16     | **0.999** | ![9-result](assets/9-result.png) | Higher decay rate affects the epsilon greedy policy score but not the policy net score. Solved the game at around 175 episodes. |

## References

1. [PyTorch Tutorial](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)
2. [pytorch-dqn](https://github.com/transedward/pytorch-dqn)
3. [DQN_pytorch](https://github.com/dxyang/DQN_pytorch)
4. [60_Days_RL_Challenge](https://github.com/andri27-ts/60_Days_RL_Challenge)

